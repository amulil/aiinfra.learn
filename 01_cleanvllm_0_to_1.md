# cleanvllm: ä» 0 åˆ° 1 æ„å»ºä¸€ä¸ª vLLM

> *"å¾é—»ä¹‹è€Œå¿˜ï¼Œè§ä¹‹è€Œè®°ï¼Œè¡Œä¹‹è€ŒçŸ¥ã€‚"*  
> â€” å­”å­

## å¼•è¨€ï¼šä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦ç†è§£ vLLMï¼Ÿ

> **è‡´è°¢**ï¼šæœ¬æ–‡åŠ cleanvllm é¡¹ç›®çš„è¯ç”Ÿè¦ç‰¹åˆ«æ„Ÿè°¢ä¸¤ä¸ªä¼˜ç§€çš„å¼€æºé¡¹ç›®ï¼š
> - **[nano-vLLM](https://github.com/GeeeekExplorer/nano-vllm)** - cleanvllm çš„æ ¸å¿ƒä»£ç å®ç°åŸºäºè¿™ä¸ªé¡¹ç›®ï¼Œå®ƒç”¨1200è¡Œä»£ç å®ç°äº†å®Œæ•´çš„ vLLM åŠŸèƒ½
> - **[CleanRL](https://github.com/vwxyzjn/cleanrl)** - cleanvllm çš„è®¾è®¡ç†å¿µå’Œæ•™è‚²å¯¼å‘æ·±å— CleanRL å¯å‘ï¼Œåè€…ä»¥å•æ–‡ä»¶å®ç°å±•ç¤ºäº†æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„ç²¾é«“
> 
> æ­£æ˜¯è¿™äº›é¡¹ç›®çš„å¼€æºç²¾ç¥ï¼Œè®©æˆ‘ä»¬èƒ½å¤Ÿç«™åœ¨å·¨äººçš„è‚©è†€ä¸Šï¼Œç”¨æ•™è‚²å‹å¥½çš„æ–¹å¼ä¼ æ’­ vLLM çš„æ ¸å¿ƒæ€æƒ³ã€‚

åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ—¶ä»£ï¼Œæ¨ç†æ•ˆç‡æˆä¸ºäº†åˆ¶çº¦åº”ç”¨è½åœ°çš„å…³é”®ç“¶é¢ˆã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œå½“ä½ å‘ ChatGPT æå‡ºé—®é¢˜æ—¶ï¼Œå®ƒéœ€è¦åœ¨å‡ ç§’å†…å¤„ç†æ•°åƒä¸ªtokenï¼ŒåŒæ—¶æœåŠ¡æ•°ä¸‡åç”¨æˆ·ã€‚è¿™èƒŒåçš„æŠ€æœ¯æŒ‘æˆ˜æ˜¯ä»€ä¹ˆï¼Ÿç­”æ¡ˆå°±æ˜¯é«˜æ•ˆçš„æ¨ç†å¼•æ“ã€‚

vLLM ä½œä¸ºå½“å‰æœ€é«˜æ•ˆçš„ LLM æ¨ç†å¼•æ“ä¹‹ä¸€ï¼Œé€šè¿‡åˆ›æ–°çš„ PagedAttentionã€KV Cache ç®¡ç†ã€è¿ç»­æ‰¹å¤„ç†ç­‰æŠ€æœ¯ï¼Œå°†æ¨ç†ååé‡æå‡äº† **24å€**ã€‚ç„¶è€Œï¼ŒvLLM çš„ç”Ÿäº§ä»£ç å¤æ‚åºå¤§ï¼ŒåŒ…å«å¤§é‡å·¥ç¨‹ä¼˜åŒ–å’Œè¾¹ç•Œæƒ…å†µå¤„ç†ï¼Œå¯¹äºåˆå­¦è€…æ¥è¯´çŠ¹å¦‚è¿·å®«ã€‚

[cleanvllm](https://github.com/amulil/cleanvllm) é¡¹ç›®åº”è¿è€Œç”Ÿâ€”â€”å®ƒæ‰¿ç»§äº† [CleanRL](https://github.com/vwxyzjn/cleanrl) çš„æ•™è‚²ç†å¿µï¼Œç”¨å•æ–‡ä»¶ï¼ˆ1500è¡ŒPythonä»£ç ï¼‰å®ç°äº† vLLM çš„æ ¸å¿ƒæ¦‚å¿µï¼Œå»é™¤äº†å·¥ç¨‹å¤æ‚æ€§ï¼Œè®©æˆ‘ä»¬èƒ½å¤Ÿä¸“æ³¨äºç†è§£æ¶æ„æœ¬è´¨ã€‚æ­£å¦‚ Linus Torvalds æ‰€è¯´ï¼š"Talk is cheap. Show me the code."

æœ¬æ–‡å°†å¸¦ä½ ä»é›¶å¼€å§‹ï¼Œé€æ­¥æ„å»ºä¸€ä¸ªå®Œæ•´çš„ vLLM æ¨ç†å¼•æ“ï¼Œæ­ç¤ºå…¶èƒŒåçš„è®¾è®¡å“²å­¦å’ŒæŠ€æœ¯ç»†èŠ‚ã€‚è¯»å®Œæœ¬æ–‡ï¼Œä½ å°†èƒ½å¤Ÿï¼š

- ç†è§£ vLLM çš„æ ¸å¿ƒæ¶æ„å’Œå…³é”®åˆ›æ–°
- æŒæ¡ PagedAttention çš„å·¥ä½œåŸç†
- å­¦ä¼šå¦‚ä½•å®ç°é«˜æ•ˆçš„ KV Cache ç®¡ç†
- äº†è§£è¿ç»­æ‰¹å¤„ç†çš„è°ƒåº¦ç­–ç•¥
- è¿è¡Œè‡ªå·±çš„ vLLM å®ç°

## ç¬¬ä¸€ç« ï¼švLLM çš„æ ¸å¿ƒæŒ‘æˆ˜ä¸è®¾è®¡æ€è·¯

### 1.1 ä¼ ç»Ÿ LLM æ¨ç†çš„ç—›ç‚¹

è®©æˆ‘ä»¬ä»¥ä¸€ä¸ªå…·ä½“çš„ä¾‹å­æ¥è¯´æ˜ï¼šå‡è®¾ä½ æœ‰ä¸€å° A100 GPUï¼ˆ80GBæ˜¾å­˜ï¼‰ï¼Œè¦åŒæ—¶å¤„ç† 100 ä¸ªç”¨æˆ·è¯·æ±‚ï¼Œæ¯ä¸ªè¯·æ±‚å¹³å‡ç”Ÿæˆ 100 ä¸ªtokenã€‚

ä¼ ç»Ÿçš„æ¨ç†æ–¹å¼ä¼šé‡åˆ°ä»¥ä¸‹é—®é¢˜ï¼š

1. **å†…å­˜ç¢ç‰‡åŒ–**ï¼šæ¯ä¸ªåºåˆ—çš„ KV Cache éœ€è¦è¿ç»­å†…å­˜ï¼Œä½†åºåˆ—é•¿åº¦ä¸åŒå¯¼è‡´ä¸¥é‡çš„å†…å­˜æµªè´¹ã€‚å®é™…å¯ç”¨å†…å­˜åªæœ‰ **60%**ï¼

2. **æ‰¹å¤„ç†å›°éš¾**ï¼šåºåˆ—Aéœ€è¦10ä¸ªtokenï¼Œåºåˆ—Béœ€è¦100ä¸ªtokenï¼Œå¦‚ä½•é«˜æ•ˆæ‰¹å¤„ç†ï¼Ÿä¼ ç»Ÿæ–¹æ³•åªèƒ½ç­‰æœ€çŸ­åºåˆ—å®Œæˆã€‚

3. **åŠ¨æ€è°ƒåº¦å¤æ‚**ï¼šä½ æ— æ³•é¢„çŸ¥ç”¨æˆ·ä¼šé—®å¤šé•¿çš„é—®é¢˜ï¼Œèµ„æºè°ƒåº¦å˜å¾—æå…¶å›°éš¾ã€‚

è¿™äº›é—®é¢˜å¯¼è‡´ GPU åˆ©ç”¨ç‡é€šå¸¸åªæœ‰ **30-40%**ï¼Œå¤§é‡ç®—åŠ›è¢«æµªè´¹ã€‚

### 1.2 vLLM çš„è§£å†³æ–¹æ¡ˆ

vLLM é€šè¿‡ä¸‰ä¸ªæ ¸å¿ƒåˆ›æ–°è§£å†³äº†è¿™äº›é—®é¢˜ï¼š

- **PagedAttention**ï¼šå°† KV Cache åˆ†é¡µç®¡ç†ï¼Œç±»ä¼¼æ“ä½œç³»ç»Ÿçš„è™šæ‹Ÿå†…å­˜
- **è¿ç»­æ‰¹å¤„ç†**ï¼šåŠ¨æ€æ·»åŠ /ç§»é™¤åºåˆ—ï¼Œæœ€å¤§åŒ– GPU åˆ©ç”¨ç‡
- **å‰ç¼€ç¼“å­˜**ï¼šå¤ç”¨ç›¸åŒå‰ç¼€çš„ KV Cacheï¼Œå‡å°‘é‡å¤è®¡ç®—

è®©æˆ‘ä»¬åœ¨ cleanvllm ä¸­çœ‹çœ‹è¿™äº›æ¦‚å¿µæ˜¯å¦‚ä½•å®ç°çš„ã€‚

## ç¬¬äºŒç« ï¼šæ•°æ®ç»“æ„è®¾è®¡ - vLLM çš„åŸºçŸ³

### 2.1 åºåˆ—ç®¡ç†ï¼šSequence ç±»

åœ¨ cleanvllm ä¸­ï¼Œæ¯ä¸ªç”¨æˆ·è¯·æ±‚è¢«æŠ½è±¡ä¸ºä¸€ä¸ª `Sequence` å¯¹è±¡ï¼š

```python
class Sequence:
    """åºåˆ—ç®¡ç†ç±» - vLLM çš„åŸºæœ¬æ‰§è¡Œå•å…ƒ"""
    block_size = 256  # æ¯ä¸ªå†…å­˜å—çš„å¤§å°
    counter = count()

    def __init__(self, token_ids: list[int], sampling_params: SamplingParams):
        self.seq_id = next(Sequence.counter)
        self.status = SequenceStatus.WAITING
        self.token_ids = copy(token_ids)
        self.last_token = token_ids[-1]
        self.num_tokens = len(self.token_ids)
        self.num_prompt_tokens = len(token_ids)
        self.num_cached_tokens = 0
        self.block_table = []  # å…³é”®ï¼šå—è¡¨ï¼Œæ˜ å°„é€»è¾‘å—åˆ°ç‰©ç†å—
        
    @property
    def num_blocks(self):
        """è®¡ç®—éœ€è¦å¤šå°‘ä¸ªå†…å­˜å—"""
        return (self.num_tokens + self.block_size - 1) // self.block_size
    
    def block(self, i):
        """è·å–ç¬¬iä¸ªå—çš„token"""
        assert 0 <= i < self.num_blocks
        return self.token_ids[i*self.block_size: (i+1)*self.block_size]
```

**å…³é”®æ´å¯Ÿ**ï¼š`block_table` æ˜¯ PagedAttention çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼Œç±»ä¼¼æ“ä½œç³»ç»Ÿçš„é¡µè¡¨ã€‚å®ƒå°†åºåˆ—çš„é€»è¾‘å†…å­˜æ˜ å°„åˆ°ç‰©ç†å†…å­˜å—ï¼Œå®ç°äº†å†…å­˜çš„çµæ´»ç®¡ç†ã€‚

### 2.2 æ‰§è¡Œä¸Šä¸‹æ–‡ï¼šContext ç±»

```python
@dataclass
class Context:
    """æ¨ç†ä¸Šä¸‹æ–‡"""
    is_prefill: bool = False
    cu_seqlens_q: torch.Tensor | None = None
    cu_seqlens_k: torch.Tensor | None = None
    max_seqlen_q: int = 0
    max_seqlen_k: int = 0
    slot_mapping: torch.Tensor | None = None
    context_lens: torch.Tensor | None = None
    block_tables: torch.Tensor | None = None
```

`Context` å­˜å‚¨å½“å‰æ¨ç†æ­¥éª¤çš„å…¨å±€çŠ¶æ€ï¼ŒåŒºåˆ† Prefillï¼ˆé¦–æ¬¡å¤„ç†æç¤ºï¼‰å’Œ Decodeï¼ˆé€tokenç”Ÿæˆï¼‰ä¸¤ä¸ªé˜¶æ®µã€‚

### 2.3 é…ç½®ç®¡ç†ï¼šConfig ç±»

```python
@dataclass
class Config:
    """å…¨å±€é…ç½®"""
    model: str
    max_num_batched_tokens: int = 32768
    max_num_seqs: int = 512
    max_model_len: int = 4096
    gpu_memory_utilization: float = 0.11
    tensor_parallel_size: int = 1
    kvcache_block_size: int = 256
    num_kvcache_blocks: int = -1
```

è¿™äº›å‚æ•°æ§åˆ¶ç€ç³»ç»Ÿçš„å†…å­˜ä½¿ç”¨ã€å¹¶å‘èƒ½åŠ›å’Œæ€§èƒ½ç‰¹å¾ã€‚

## ç¬¬ä¸‰ç« ï¼šå†…å­˜ç®¡ç† - PagedAttention çš„å®ç°

### 3.1 å†…å­˜å—æŠ½è±¡

```python
class Block:
    """å†…å­˜å—ç±»"""
    def __init__(self, block_id):
        self.block_id = block_id
        self.ref_count = 0
        self.hash = -1
        self.token_ids = []

    def update(self, hash: int, token_ids: list[int]):
        assert hash != -1
        self.hash = hash
        self.token_ids = token_ids
```

æ¯ä¸ª `Block` ä»£è¡¨ä¸€ä¸ªå›ºå®šå¤§å°ï¼ˆé€šå¸¸256ä¸ªtokenï¼‰çš„å†…å­˜å—ï¼Œå¯ä»¥å­˜å‚¨ KV Cacheã€‚`hash` å­—æ®µæ”¯æŒå‰ç¼€ç¼“å­˜ä¼˜åŒ–ã€‚

### 3.2 å—ç®¡ç†å™¨ï¼šPagedAttention çš„æ ¸å¿ƒ

```python
class BlockManager:
    """å†…å­˜å—ç®¡ç†å™¨"""
    def __init__(self, num_blocks: int, block_size: int):
        self.block_size = block_size
        self.blocks: list[Block] = [Block(i) for i in range(num_blocks)]
        self.hash_to_block_id: dict[int, int] = dict()
        self.free_block_ids: deque[int] = deque(range(num_blocks))
        self.used_block_ids: set[int] = set()

    def allocate(self, seq: Sequence):
        """ä¸ºåºåˆ—åˆ†é…å†…å­˜å—"""
        assert not seq.block_table
        h = -1
        cache_miss = False
        for i in range(seq.num_blocks):
            token_ids = seq.block(i)
            h = compute_hash(token_ids, h) if len(token_ids) == self.block_size else -1
            block_id = self.hash_to_block_id.get(h, -1)
            if block_id == -1 or self.blocks[block_id].token_ids != token_ids:
                cache_miss = True
            if cache_miss:
                block_id = self.free_block_ids[0]
                block = self._allocate_block(block_id)
            else:
                seq.num_cached_tokens += self.block_size
                # å‰ç¼€ç¼“å­˜å‘½ä¸­ï¼
                if block_id in self.used_block_ids:
                    block = self.blocks[block_id]
                    block.ref_count += 1
                else:
                    block = self._allocate_block(block_id)
            seq.block_table.append(block_id)
```

è¿™æ˜¯ PagedAttention çš„æ ¸å¿ƒå®ç°ï¼š
- **åˆ†é¡µç®¡ç†**ï¼šKV Cache è¢«åˆ†å‰²æˆå›ºå®šå¤§å°çš„å—
- **å‰ç¼€ç¼“å­˜**ï¼šé€šè¿‡å“ˆå¸Œå€¼æ£€æµ‹ç›¸åŒçš„ token åºåˆ—ï¼Œå®ç° KV Cache å¤ç”¨
- **å¼•ç”¨è®¡æ•°**ï¼šæ”¯æŒå¤šä¸ªåºåˆ—å…±äº«ç›¸åŒçš„å†…å­˜å—

### 3.3 å“ˆå¸Œè®¡ç®—ï¼šå‰ç¼€ç¼“å­˜çš„å…³é”®

```python
def compute_hash(token_ids: list[int], prefix: int = -1):
    """è®¡ç®—tokenåºåˆ—çš„å“ˆå¸Œå€¼"""
    h = xxhash.xxh64()
    if prefix != -1:
        h.update(prefix.to_bytes(8, "little"))
    h.update(np.array(token_ids).tobytes())
    return h.intdigest()
```

é€šè¿‡å¢é‡å“ˆå¸Œè®¡ç®—ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°è¯†åˆ«ç›¸åŒçš„ token å‰ç¼€ï¼Œå®ç° KV Cache çš„è·¨åºåˆ—å¤ç”¨ã€‚

## ç¬¬å››ç« ï¼šè°ƒåº¦å™¨ - è¿ç»­æ‰¹å¤„ç†çš„å®ç°

### 4.1 è°ƒåº¦ç­–ç•¥

```python
class Scheduler:
    """åºåˆ—è°ƒåº¦å™¨"""
    def schedule(self) -> tuple[list[Sequence], bool]:
        # Prefillé˜¶æ®µ
        scheduled_seqs = []
        num_seqs = 0
        num_batched_tokens = 0
        
        while self.waiting and num_seqs < self.max_num_seqs:
            seq = self.waiting[0]
            if num_batched_tokens + len(seq) > self.max_num_batched_tokens or not self.block_manager.can_allocate(seq):
                break
            num_seqs += 1
            self.block_manager.allocate(seq)
            num_batched_tokens += len(seq) - seq.num_cached_tokens
            seq.status = SequenceStatus.RUNNING
            self.waiting.popleft()
            self.running.append(seq)
            scheduled_seqs.append(seq)
            
        if scheduled_seqs:
            return scheduled_seqs, True

        # Decodeé˜¶æ®µ
        while self.running and num_seqs < self.max_num_seqs:
            seq = self.running.popleft()
            while not self.block_manager.can_append(seq):
                if self.running:
                    self.preempt(self.running.pop())
                else:
                    self.preempt(seq)
                    break
            else:
                num_seqs += 1
                self.block_manager.may_append(seq)
                scheduled_seqs.append(seq)
```

è°ƒåº¦å™¨å®ç°äº†è¿ç»­æ‰¹å¤„ç†çš„æ ¸å¿ƒé€»è¾‘ï¼š
- **ä¼˜å…ˆ Prefill**ï¼šæ–°åºåˆ—ä¼˜å…ˆå¤„ç†ï¼Œå……åˆ†åˆ©ç”¨å¹¶è¡Œè®¡ç®—
- **åŠ¨æ€è°ƒæ•´**ï¼šæ ¹æ®å†…å­˜å’Œè®¡ç®—èµ„æºåŠ¨æ€æ·»åŠ /ç§»é™¤åºåˆ—
- **æŠ¢å æœºåˆ¶**ï¼šåœ¨èµ„æºä¸è¶³æ—¶æš‚åœä½ä¼˜å…ˆçº§åºåˆ—

### 4.2 åºåˆ—çŠ¶æ€ç®¡ç†

```python
class SequenceStatus(Enum):
    """åºåˆ—çŠ¶æ€æšä¸¾"""
    WAITING = auto()
    RUNNING = auto()
    FINISHED = auto()
```

åºåˆ—åœ¨ä¸‰ä¸ªçŠ¶æ€é—´è½¬æ¢ï¼š
- `WAITING` â†’ `RUNNING`ï¼šè°ƒåº¦å™¨åˆ†é…èµ„æº
- `RUNNING` â†’ `FINISHED`ï¼šç”Ÿæˆå®Œæˆ
- `RUNNING` â†’ `WAITING`ï¼šè¢«æŠ¢å ï¼Œé‡Šæ”¾èµ„æº

## ç¬¬äº”ç« ï¼šæ³¨æ„åŠ›æœºåˆ¶ - PagedAttention çš„è®¡ç®—

### 5.1 KV Cache å­˜å‚¨

```python
@triton.jit
def store_kvcache_kernel(
    key_ptr,
    key_stride,
    value_ptr,
    value_stride,
    k_cache_ptr,
    v_cache_ptr,
    slot_mapping_ptr,
    D: tl.constexpr,
):
    """Triton kernel for storing KV cache"""
    idx = tl.program_id(0)
    key_offsets = idx * key_stride + tl.arange(0, D)
    value_offsets = idx * value_stride + tl.arange(0, D)
    key = tl.load(key_ptr + key_offsets)
    value = tl.load(value_ptr + value_offsets)
    slot = tl.load(slot_mapping_ptr + idx)
    cache_offsets = slot * D + tl.arange(0, D)
    tl.store(k_cache_ptr + cache_offsets, key)
    tl.store(v_cache_ptr + cache_offsets, value)
```

è¿™ä¸ª Triton å†…æ ¸å®ç°äº†é«˜æ•ˆçš„ KV Cache å­˜å‚¨ï¼š
- `slot_mapping` å°†é€»è¾‘ä½ç½®æ˜ å°„åˆ°ç‰©ç†å†…å­˜ä½ç½®
- æ”¯æŒéè¿ç»­çš„å†…å­˜è®¿é—®æ¨¡å¼
- åˆ©ç”¨ GPU çš„å¹¶è¡Œæ€§åŠ é€Ÿå­˜å‚¨æ“ä½œ

### 5.2 æ³¨æ„åŠ›è®¡ç®—

```python
class Attention(nn.Module):
    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor):
        context = get_context()
        k_cache = self.k_cache
        v_cache = self.v_cache
        store_kvcache(k, v, k_cache, v_cache, context.slot_mapping)
        
        if HAS_FLASH_ATTN:
            if context.is_prefill:
                if context.block_tables is not None:
                    # ä½¿ç”¨åˆ†é¡µæ³¨æ„åŠ›
                    o = flash_attn_varlen_func(q, k_cache, v_cache,
                                               max_seqlen_q=context.max_seqlen_q, cu_seqlens_q=context.cu_seqlens_q,
                                               max_seqlen_k=context.max_seqlen_k, cu_seqlens_k=context.cu_seqlens_k,
                                               softmax_scale=self.scale, causal=True, block_table=context.block_tables)
                else:
                    # ä½¿ç”¨å¸¸è§„æ³¨æ„åŠ›
                    o = flash_attn_varlen_func(q, k, v, ...)
            else:
                # Decodeé˜¶æ®µä½¿ç”¨KV Cache
                o = flash_attn_with_kvcache(q.unsqueeze(1), k_cache, v_cache,
                                            cache_seqlens=context.context_lens, block_table=context.block_tables, 
                                            softmax_scale=self.scale, causal=True)
```

æ³¨æ„åŠ›æœºåˆ¶æ ¹æ®æ‰§è¡Œé˜¶æ®µé€‰æ‹©ä¸åŒçš„è®¡ç®—è·¯å¾„ï¼š
- **Prefill + æ— ç¼“å­˜**ï¼šæ ‡å‡†çš„å˜é•¿æ³¨æ„åŠ›
- **Prefill + æœ‰ç¼“å­˜**ï¼šåˆ†é¡µæ³¨æ„åŠ›ï¼Œåˆ©ç”¨ç¼“å­˜çš„ KV
- **Decode**ï¼šåŸºäº KV Cache çš„å•æ­¥æ³¨æ„åŠ›

## ç¬¬å…­ç« ï¼šæ¨¡å‹æ¶æ„ - Qwen3 çš„å®ç°

### 6.1 å¹¶è¡Œçº¿æ€§å±‚

cleanvllm å®ç°äº†å®Œæ•´çš„å¼ é‡å¹¶è¡Œæ”¯æŒï¼š

```python
class QKVParallelLinear(ColumnParallelLinear):
    """QKVå¹¶è¡Œçº¿æ€§å±‚"""
    def __init__(self, hidden_size: int, head_size: int, total_num_heads: int, 
                 total_num_kv_heads: int | None = None, bias: bool = False):
        # è®¡ç®—æ¯ä¸ªGPUçš„å¤´æ•°
        tp_size = dist.get_world_size() if dist.is_initialized() else 1
        self.num_heads = divide(self.total_num_heads, tp_size)
        self.num_kv_heads = divide(self.total_num_kv_heads, tp_size)
        
        # åˆ†å‰²æƒé‡
        output_size = (self.num_heads + 2 * self.num_kv_heads) * tp_size * self.head_size
        super().__init__(input_size, output_size, bias)
```

### 6.2 Qwen3 æ³¨æ„åŠ›å±‚

```python
class Qwen3Attention(nn.Module):
    def forward(self, positions: torch.Tensor, hidden_states: torch.Tensor) -> torch.Tensor:
        qkv = self.qkv_proj(hidden_states)
        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
        
        # åº”ç”¨ RMSNormï¼ˆQwen3 ç‰¹æœ‰ï¼‰
        q_by_head = q.view(-1, self.num_heads, self.head_dim)
        q_by_head = self.q_norm(q_by_head)
        k_by_head = k.view(-1, self.num_kv_heads, self.head_dim)
        k_by_head = self.k_norm(k_by_head)
        
        # åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç 
        q, k = self.rotary_emb(positions, q, k)
        
        # æ‰§è¡Œæ³¨æ„åŠ›è®¡ç®—
        attn_output = self.attn(q, k, v)
        output = self.o_proj(attn_output)
        return output
```

## ç¬¬ä¸ƒç« ï¼šæ¨¡å‹è¿è¡Œå™¨ - æ¨ç†çš„æ‰§è¡Œå¼•æ“

### 7.1 KV Cache åˆ†é…

```python
def allocate_kv_cache(self, gpu_memory_utilization: float):
    """åˆ†é… KV cache"""
    total_memory, used_memory, free_memory = get_gpu_memory()
    
    # è®¡ç®—æ¯ä¸ªå—çš„å†…å­˜å¤§å°
    head_dim = getattr(self.config.hf_config, 'head_dim', None) or self.config.hf_config.hidden_size // self.config.hf_config.num_attention_heads
    num_kv_heads = getattr(self.config.hf_config, 'num_key_value_heads', self.config.hf_config.num_attention_heads)
    num_kv_heads_per_gpu = num_kv_heads // self.world_size
    
    # å†…å­˜æ¯å— = block_size * head_dim * num_kv_heads_per_gpu * 2 (Kå’ŒV) * 2 (float16) * num_layers
    bytes_per_block = self.block_size * head_dim * num_kv_heads_per_gpu * 2 * 2 * len(self.model.model.layers)
    
    # æ ¹æ®å¯ç”¨å†…å­˜è®¡ç®—å—æ•°
    available_memory = free_memory * gpu_memory_utilization
    num_blocks = max(1, int(available_memory // bytes_per_block))
```

è¿™ä¸ªå‡½æ•°åŠ¨æ€è®¡ç®— KV Cache çš„å¤§å°ï¼Œå……åˆ†åˆ©ç”¨å¯ç”¨ GPU å†…å­˜ã€‚

### 7.2 CUDA Graph ä¼˜åŒ–

```python
def capture_cudagraph(self):
    """æ•è· CUDA graph"""
    max_batch_size = 512
    self.graphs = {}
    self.graph_vars = {}
    self.graph_bs = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]
    
    for bs in self.graph_bs:
        graph = torch.cuda.CUDAGraph()
        graph_vars = {}
        # é¢„åˆ†é…è¾“å…¥å¼ é‡
        graph_vars["input_ids"] = torch.zeros(max_batch_size, dtype=torch.int64, device="cuda")
        graph_vars["positions"] = torch.zeros(max_batch_size, dtype=torch.int64, device="cuda")
        
        with torch.cuda.graph(graph):
            graph_vars["outputs"] = self.model(
                graph_vars["input_ids"][:bs], 
                graph_vars["positions"][:bs]
            )
        
        self.graphs[bs] = graph
        self.graph_vars = graph_vars
```

CUDA Graph é€šè¿‡é¢„ç¼–è¯‘è®¡ç®—å›¾æ˜¾è‘—å‡å°‘ kernel å¯åŠ¨å¼€é”€ï¼Œç‰¹åˆ«æ˜¯åœ¨ Decode é˜¶æ®µæ•ˆæœæ˜æ˜¾ã€‚

## ç¬¬å…«ç« ï¼šLLM å¼•æ“ - ç»Ÿä¸€çš„æ¨ç†æ¥å£

### 8.1 å¼•æ“åˆå§‹åŒ–

```python
class LLMEngine:
    def __init__(self, model, **kwargs):
        config = Config(model, **kwargs)
        
        # å¯åŠ¨å¤šè¿›ç¨‹å¼ é‡å¹¶è¡Œ
        self.ps = []
        self.events = []
        for i in range(1, config.tensor_parallel_size):
            event = mp.Event()
            process = mp.Process(target=ModelRunner, args=(config, i, event))
            process.start()
            self.ps.append(process)
            self.events.append(event)
        
        self.model_runner = ModelRunner(config, 0, self.events)
        self.tokenizer = AutoTokenizer.from_pretrained(config.model)
        self.scheduler = Scheduler(config)
```

### 8.2 ç”Ÿæˆæµç¨‹

```python
def generate(self, prompts: list[str], sampling_params: SamplingParams) -> list[str]:
    """ç”Ÿæˆæ–‡æœ¬"""
    # æ·»åŠ è¯·æ±‚åˆ°è°ƒåº¦å™¨
    for prompt, sp in zip(prompts, sampling_params):
        self.add_request(prompt, sp)
    
    outputs = {}
    while not self.is_finished():
        # æ‰§è¡Œä¸€æ­¥æ¨ç†
        output, num_tokens = self.step()
        for seq_id, token_ids in output:
            outputs[seq_id] = token_ids
    
    # è§£ç è¾“å‡º
    outputs = [outputs[seq_id] for seq_id in sorted(outputs)]
    return [{"text": self.tokenizer.decode(token_ids), "token_ids": token_ids} for token_ids in outputs]

def step(self):
    """æ‰§è¡Œä¸€æ­¥æ¨ç†"""
    seqs, is_prefill = self.scheduler.schedule()
    token_ids = self.model_runner.call("run", seqs, is_prefill)
    self.scheduler.postprocess(seqs, token_ids)
    return outputs, num_tokens
```

## ç¬¬ä¹ç« ï¼šæ€§èƒ½ä¼˜åŒ–æŠ€æœ¯

### 9.1 Flash Attention é›†æˆ

cleanvllm ä¼˜é›…åœ°é›†æˆäº† Flash Attentionï¼Œå¹¶æä¾›äº† PyTorch åŸç”Ÿçš„å›é€€å®ç°ï¼š

```python
try:
    from flash_attn import flash_attn_varlen_func, flash_attn_with_kvcache
    HAS_FLASH_ATTN = True
    print("Flash Attention available, using optimized attention")
except ImportError:
    HAS_FLASH_ATTN = False
    print("Flash Attention not available, using PyTorch native attention")
```

è¿™ç§è®¾è®¡ä¿è¯äº†ä»£ç çš„å¯ç§»æ¤æ€§ï¼Œå³ä½¿åœ¨æ²¡æœ‰ Flash Attention çš„ç¯å¢ƒä¸­ä¹Ÿèƒ½æ­£å¸¸è¿è¡Œã€‚

### 9.2 Triton å†…æ ¸ä¼˜åŒ–

ä½¿ç”¨ Triton ç¼–å†™çš„ KV Cache å­˜å‚¨å†…æ ¸å±•ç¤ºäº†å¦‚ä½•åœ¨ Python ä¸­ç¼–å†™é«˜æ€§èƒ½ GPU ä»£ç ï¼š

```python
@triton.jit
def store_kvcache_kernel(...):
    """é«˜æ•ˆçš„KV Cacheå­˜å‚¨å†…æ ¸"""
    # åˆ©ç”¨GPUçš„å¹¶è¡Œæ€§å’Œå†…å­˜å±‚æ¬¡ç»“æ„
    # æ¯”çº¯PyTorchå®ç°å¿«æ•°å€
```

### 9.3 å†…å­˜ä¼˜åŒ–ç­–ç•¥

1. **åˆ†é¡µç®¡ç†**ï¼šé¿å…å†…å­˜ç¢ç‰‡åŒ–
2. **å‰ç¼€ç¼“å­˜**ï¼šå‡å°‘é‡å¤è®¡ç®—
3. **åŠ¨æ€åˆ†é…**ï¼šæ ¹æ®å®é™…éœ€æ±‚è°ƒæ•´å†…å­˜ä½¿ç”¨
4. **å¼•ç”¨è®¡æ•°**ï¼šæ”¯æŒå†…å­˜å—å…±äº«

## ç¬¬åç« ï¼šå®æˆ˜æ¼”ç¤º - è¿è¡Œä½ çš„ç¬¬ä¸€ä¸ª vLLM

è®©æˆ‘ä»¬è¿è¡Œä¸€ä¸ªå®Œæ•´çš„ä¾‹å­ï¼Œçœ‹çœ‹ cleanvllm å¦‚ä½•å·¥ä½œï¼š

### 10.1 å¿«é€Ÿå¼€å§‹

```python
#!/usr/bin/env python3
import os
from qwen3_0_6B import LLM, SamplingParams

if __name__ == "__main__":
    # 1. æ¨¡å‹è·¯å¾„é…ç½®ï¼ˆä¿®æ”¹ä¸ºä½ çš„æ¨¡å‹è·¯å¾„ï¼‰
    path = os.path.expanduser("~/model/qwen/Qwen3-0.6B")
    
    # 2. åˆå§‹åŒ–å¼•æ“
    print("ğŸš€ Initializing cleanvLLM engine...")
    llm = LLM(
        path, 
        enforce_eager=True,           # ç¦ç”¨CUDA Graphï¼Œä¾¿äºè°ƒè¯•
        gpu_memory_utilization=0.10,  # ä½¿ç”¨10%çš„GPUå†…å­˜
        tensor_parallel_size=1        # å•GPUæ¨¡å¼
    )
    
    # 3. è®¾ç½®é‡‡æ ·å‚æ•°
    sampling_params = SamplingParams(
        temperature=0.6,   # åˆ›é€ æ€§
        max_tokens=256     # æœ€å¤§ç”Ÿæˆé•¿åº¦
    )
    
    # 4. å‡†å¤‡å¤šä¸ªæç¤ºï¼ˆæ¼”ç¤ºæ‰¹å¤„ç†ï¼‰
    prompts = [
        "Explain quantum computing in simple terms",
        "Write a Python function to sort a list", 
        "What are the benefits of renewable energy?"
    ]
    
    # 5. æ‰§è¡Œæ¨ç†
    print("âš¡ Starting inference...")
    outputs = llm.generate(prompts, sampling_params)
    
    # 6. æ˜¾ç¤ºç»“æœ
    for i, (prompt, output) in enumerate(zip(prompts, outputs)):
        print(f"\n{'='*60}")
        print(f"ğŸ“ Prompt {i+1}: {prompt}")
        print(f"ğŸ¤– Response: {output['text']}")
        print(f"ğŸ“Š Tokens: {len(output['token_ids'])}")
```

### 10.2 æ€§èƒ½åˆ†æ

è¿è¡Œä¸Šè¿°ä»£ç ï¼Œä½ ä¼šçœ‹åˆ°ç±»ä¼¼çš„è¾“å‡ºï¼š

```
ğŸš€ Initializing cleanvLLM engine...
GPU Memory: Total 23.0GB, Used 2.0GB, Free 21.0GB
KV Cache Allocation: 79 blocks, 28.0MB per block, 2212.0MB total
Model initialization complete!

âš¡ Starting inference...
Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:12<00:00, 4.2s/it, Prefill=156tok/s, Decode=42tok/s]

====================================================
ğŸ“ Prompt 1: Explain quantum computing in simple terms
ğŸ¤– Response: Quantum computing is like having a super-powerful...
ğŸ“Š Tokens: 128

====================================================
ğŸ“ Prompt 2: Write a Python function to sort a list
ğŸ¤– Response: Here's a simple Python function that sorts a list...
ğŸ“Š Tokens: 94
```

**æ€§èƒ½æŒ‡æ ‡è§£è¯»**ï¼š
- **Prefillé€Ÿåº¦**: 156 tokens/ç§’ - å¤„ç†è¾“å…¥æç¤ºçš„é€Ÿåº¦
- **Decodeé€Ÿåº¦**: 42 tokens/ç§’ - ç”Ÿæˆæ–°tokençš„é€Ÿåº¦
- **å†…å­˜ä½¿ç”¨**: 2.2GB KV Cacheï¼Œé«˜æ•ˆåˆ©ç”¨äº†GPUå†…å­˜

### 10.3 å¹•åå‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ

è¿™ä¸ªç®€å•çš„æ¥å£èƒŒåè¿è¡Œç€å®Œæ•´çš„ vLLM æ¨ç†æµç¨‹ï¼š

1. **è°ƒåº¦å™¨**ï¼šå°†3ä¸ªè¯·æ±‚åŠ å…¥ç­‰å¾…é˜Ÿåˆ—
2. **å†…å­˜ç®¡ç†**ï¼šä¸ºæ¯ä¸ªåºåˆ—åˆ†é…å†…å­˜å—
3. **Prefillé˜¶æ®µ**ï¼šå¹¶è¡Œå¤„ç†æ‰€æœ‰è¾“å…¥æç¤º
4. **Decodeé˜¶æ®µ**ï¼šé€tokenç”Ÿæˆï¼ŒåŠ¨æ€è°ƒåº¦
5. **KV Cache**ï¼šç¼“å­˜æ³¨æ„åŠ›çŠ¶æ€ï¼Œé¿å…é‡å¤è®¡ç®—
6. **ä¼˜åŒ–å™¨**ï¼šä½¿ç”¨Flash AttentionåŠ é€Ÿè®¡ç®—

æ•´ä¸ªè¿‡ç¨‹å±•ç°äº† vLLM çš„æ ¸å¿ƒä¼˜åŠ¿ï¼š**é«˜ååé‡çš„å¹¶è¡Œæ¨ç†**ã€‚

### 10.4 vLLM æ¶æ„æ€»è§ˆ

è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªå›¾æ¥æ€»ç»“ cleanvLLM çš„å®Œæ•´æ¶æ„ï¼š

```
                    User Requests
                         â”‚
                         â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ LLMEngine   â”‚ â†â”€â”€ ç”¨æˆ·æ¥å£
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ Scheduler   â”‚ â†â”€â”€ è¿ç»­æ‰¹å¤„ç†è°ƒåº¦
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚BlockManager â”‚ â†â”€â”€ PagedAttentionå†…å­˜ç®¡ç†
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ ModelRunner â”‚ â†â”€â”€ æ¨¡å‹æ‰§è¡Œå¼•æ“
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ Qwen3Model  â”‚ â†â”€â”€ Transformeræ¶æ„
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                 GPU Computation
          (Flash Attention + CUDA Graph)
```

æ¯ä¸€å±‚éƒ½æœ‰å…¶ç‹¬ç‰¹çš„èŒè´£ï¼Œå…±åŒæ„æˆäº†é«˜æ•ˆçš„æ¨ç†å¼•æ“ã€‚

## æ€»ç»“ï¼švLLM çš„è®¾è®¡å“²å­¦

é€šè¿‡ cleanvllm çš„å®ç°ï¼Œæˆ‘ä»¬å¯ä»¥æ€»ç»“å‡º vLLM çš„å‡ ä¸ªæ ¸å¿ƒè®¾è®¡åŸåˆ™ï¼š

### 1. åˆ†å±‚æŠ½è±¡
- **åº•å±‚**ï¼šå†…å­˜ç®¡ç†ã€è®¡ç®—å†…æ ¸
- **ä¸­å±‚**ï¼šè°ƒåº¦å™¨ã€æ¨¡å‹è¿è¡Œå™¨
- **ä¸Šå±‚**ï¼šç»Ÿä¸€çš„æ¨ç†æ¥å£

### 2. æ¨¡å—åŒ–è®¾è®¡
æ¯ä¸ªç»„ä»¶èŒè´£æ¸…æ™°ï¼Œä¾¿äºç†è§£å’Œæ‰©å±•ï¼š
- `BlockManager`ï¼šå†…å­˜ç®¡ç†
- `Scheduler`ï¼šä»»åŠ¡è°ƒåº¦
- `ModelRunner`ï¼šæ¨¡å‹æ‰§è¡Œ
- `LLMEngine`ï¼šç”¨æˆ·æ¥å£

### 3. æ€§èƒ½ä¼˜å…ˆ
- ä½¿ç”¨æœ€å…ˆè¿›çš„ä¼˜åŒ–æŠ€æœ¯ï¼ˆFlash Attentionã€CUDA Graphã€Tritonï¼‰
- æä¾›å›é€€å®ç°ä¿è¯å…¼å®¹æ€§
- å……åˆ†åˆ©ç”¨ç¡¬ä»¶ç‰¹æ€§

### 4. å¯æ‰©å±•æ€§
- æ”¯æŒå¼ é‡å¹¶è¡Œ
- æ”¯æŒå¤šç§æ¨¡å‹æ¶æ„
- æ”¯æŒä¸åŒçš„ä¼˜åŒ–ç­–ç•¥

cleanvllm è™½ç„¶æ˜¯ä¸€ä¸ªæ•™è‚²é¡¹ç›®ï¼Œä½†å®ƒæˆåŠŸåœ°å±•ç¤ºäº† vLLM çš„æ ¸å¿ƒæ€æƒ³ã€‚é€šè¿‡ç†è§£è¿™äº›æ¦‚å¿µï¼Œæˆ‘ä»¬å¯ä»¥æ›´å¥½åœ°ä½¿ç”¨å’Œä¼˜åŒ– LLM æ¨ç†ç³»ç»Ÿï¼Œä¸º AI åº”ç”¨çš„å¤§è§„æ¨¡éƒ¨ç½²å¥ å®šåŸºç¡€ã€‚

## ä¸‹ä¸€æ­¥ï¼šä»å­¦ä¹ åˆ°å®è·µ

### ä¸ºå·¥ç¨‹å¸ˆçš„å»ºè®®

1. **ç†è§£åŸç†**ï¼šå…ˆææ‡‚ PagedAttention å’Œè¿ç»­æ‰¹å¤„ç†çš„åŸç†
2. **é˜…è¯»ä»£ç **ï¼šä»”ç»†ç ”è¯» cleanvllm çš„å®ç°ï¼Œç‰¹åˆ«æ˜¯ `BlockManager` å’Œ `Scheduler`
3. **å®éªŒä¼˜åŒ–**ï¼šå°è¯•ä¸åŒçš„é…ç½®å‚æ•°ï¼Œè§‚å¯Ÿæ€§èƒ½å˜åŒ–
4. **æ‰©å±•åŠŸèƒ½**ï¼šä¸º cleanvllm æ·»åŠ æ–°ç‰¹æ€§ï¼Œå¦‚æŠ•æœºè§£ç ã€é‡åŒ–æ”¯æŒ

### ä¸ºç ”ç©¶è€…çš„å»ºè®®

1. **æ·±å…¥è®ºæ–‡**ï¼šé˜…è¯» vLLM åŸå§‹è®ºæ–‡ï¼Œç†è§£ç†è®ºåŸºç¡€
2. **æ€§èƒ½åˆ†æ**ï¼šä½¿ç”¨ NVIDIA Nsight åˆ†æ GPU åˆ©ç”¨ç‡
3. **ç®—æ³•æ”¹è¿›**ï¼šæ¢ç´¢æ›´å¥½çš„è°ƒåº¦ç®—æ³•å’Œå†…å­˜ç®¡ç†ç­–ç•¥
4. **æ–°çš„åº”ç”¨**ï¼šå°† vLLM çš„æ€æƒ³åº”ç”¨åˆ°å…¶ä»–æ¨ç†åœºæ™¯

### å¸¸è§é—®é¢˜è§£ç­”

**Q: ä¸ºä»€ä¹ˆ vLLM æ¯”ä¼ ç»Ÿæ¨ç†å¿«è¿™ä¹ˆå¤šï¼Ÿ**
A: ä¸»è¦æ˜¯ä¸‰ä¸ªåŸå› ï¼šPagedAttention æ¶ˆé™¤äº†å†…å­˜ç¢ç‰‡åŒ–ï¼Œè¿ç»­æ‰¹å¤„ç†æé«˜äº† GPU åˆ©ç”¨ç‡ï¼ŒFlash Attention ä¼˜åŒ–äº†è®¡ç®—æ•ˆç‡ã€‚

**Q: cleanvllm èƒ½ç”¨äºç”Ÿäº§ç¯å¢ƒå—ï¼Ÿ**
A: cleanvllm ä¸»è¦ç”¨äºæ•™è‚²å’Œç†è§£ï¼Œç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨å®˜æ–¹ vLLMï¼Œå®ƒæœ‰æ›´å¤šä¼˜åŒ–å’Œé”™è¯¯å¤„ç†ã€‚

**Q: å¦‚ä½•ä¸ºè‡ªå·±çš„æ¨¡å‹é€‚é… vLLMï¼Ÿ**
A: å…³é”®æ˜¯å®ç°æ¨¡å‹çš„å‰å‘ä¼ æ’­å’Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œå‚è€ƒ cleanvllm ä¸­çš„ `Qwen3Attention` ç±»ã€‚

## æ‰©å±•é˜…è¯»

### æ ¸å¿ƒè®ºæ–‡
- [vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention](https://arxiv.org/abs/2309.06180)
- [FlashAttention: Fast and Memory-Efficient Exact Attention](https://arxiv.org/abs/2205.14135)
- [Efficient Memory Management for Large Language Model Serving](https://arxiv.org/abs/2309.06180)

### æŠ€æœ¯åšå®¢
- [vLLM å®˜æ–¹åšå®¢ï¼šPagedAttention è§£æ](https://blog.vllm.ai/2023/06/20/vllm.html)
- [æ·±å…¥ç†è§£ KV Cache ä¼˜åŒ–](https://huggingface.co/blog/llama2#how-is-llama-2-different-from-llama-1)
- [Triton ç¼–ç¨‹æŒ‡å—](https://triton-lang.org/main/getting-started/tutorials/index.html)

### ç›¸å…³é¡¹ç›®
- [vLLM å®˜æ–¹ä»“åº“](https://github.com/vllm-project/vllm)
- [FlashAttention å®ç°](https://github.com/Dao-AILab/flash-attention)
- [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)

---

**è‡´è°¢**ï¼šæœ¬æ–‡åŸºäº [cleanvllm](https://github.com/amulil/cleanvllm) é¡¹ç›®ï¼Œè‡´åŠ›äºè®©æ›´å¤šäººç†è§£ vLLM çš„è®¾è®¡ç²¾é«“ã€‚ç‰¹åˆ«æ„Ÿè°¢ï¼š

- **[nano-vLLM](https://github.com/GeeeekExplorer/nano-vllm)** é¡¹ç›®æä¾›äº†æ ¸å¿ƒä»£ç å®ç°ï¼Œè¯æ˜äº†ç”¨ç®€æ´ä»£ç å®ç°å¤æ‚ç³»ç»Ÿçš„å¯èƒ½æ€§
- **[CleanRL](https://github.com/vwxyzjn/cleanrl)** é¡¹ç›®çš„æ•™è‚²ç†å¿µå¯å‘ï¼Œå±•ç¤ºäº†å¦‚ä½•ç”¨å•æ–‡ä»¶å®ç°è®©å¤æ‚ç®—æ³•å˜å¾—æ˜“äºç†è§£
- **vLLM å›¢é˜Ÿ**çš„å¼€åˆ›æ€§å·¥ä½œï¼Œä¸º LLM æ¨ç†å¼•æ“æ ‘ç«‹äº†æ–°çš„æ ‡æ†
- æ‰€æœ‰ä¸ºå¼€æºç¤¾åŒºè´¡çŒ®çš„å¼€å‘è€…ä»¬ï¼Œæ­£æ˜¯ä½ ä»¬çš„æ— ç§åˆ†äº«æ¨åŠ¨äº†æŠ€æœ¯çš„è¿›æ­¥

*"The best way to understand a system is to build it yourself."* â€” å¸Œæœ›è¿™ç¯‡æ–‡ç« èƒ½å¸®åŠ©ä½ æ·±å…¥ç†è§£ vLLM çš„é­…åŠ›ï¼